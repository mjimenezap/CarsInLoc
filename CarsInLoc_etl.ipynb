{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebooks aims to include the data cleansing stage and to check the feasibility of two use cases: \n",
    "# - Predict the location of a certain car for given time stamps\n",
    "# - Predict the number of cars on a certain location for given time stamps\n",
    "\n",
    "# The first use case seems not feasible because the location data is too randomly distributed across the city\n",
    "# The second use case shows a kind of pattern that can be suitable for ML and DL algorithms\n",
    "\n",
    "# Therefore, only the second use case will be continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBRARIES\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "from scipy.stats import kde\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import os \n",
    "dirpath = os.getcwd()\n",
    "\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start working with the first use case, it is necessary to extract the different car labels and copy them into\n",
    "# different columns. Otherwise, in the format they are saved it is going to be impossible to filter correctly\n",
    "\n",
    "# Therefore, the first stage is to copy all the label data in some .csv files \n",
    "\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"sample_table.csv\")\n",
    "cars_list  = df.select(\"carsList\").rdd.flatMap(lambda x: x).collect()\n",
    "small_list_size = 1000000\n",
    "cars_small_lists = [cars_list[x:x+small_list_size] for x in range(0, len(cars_list), small_list_size )] #Divide cars list into smaller lists\n",
    "max_size = 0\n",
    "number_file = 0\n",
    "\n",
    "for small_list in cars_small_lists: \n",
    "    number_file = number_file + 1\n",
    "\n",
    "    with open(\"cars_expanded_files/cars_expanded_\"+str(number_file)+\".csv\", \"a\", newline=\"\") as f:\n",
    "        wr = csv.writer(f, dialect='excel')\n",
    "    \n",
    "        for row in small_list: #For each row of the spark dataframe\n",
    "            \n",
    "            row_wr = []\n",
    "\n",
    "            row = tuple(filter(None, row.split(',')))\n",
    "            \n",
    "            if len(row)>max_size:\n",
    "                max_size = len(row)\n",
    "                \n",
    "            row_cleaned = []\n",
    "\n",
    "            for element in row: \n",
    "                #remove [] and spaces\n",
    "                element = element.replace('[', '')\n",
    "                element = element.replace(']', '')\n",
    "                element = element.replace(' ', '')\n",
    "                row_cleaned.append(element)\n",
    "            \n",
    "            row_wr.append(row_cleaned)\n",
    "            \n",
    "            wr.writerows(row_wr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we have the csv files, the labels are registered in the same timestamp in different columns\n",
    "# This is saved into a new dataframe called 'appended'\n",
    "\n",
    "paths = [dirpath + \"/cars_expanded_files/cars_expanded_1.csv\",\\\n",
    "         dirpath + \"/cars_expanded_files/cars_expanded_2.csv\",\\\n",
    "         dirpath + \"/cars_expanded_files/cars_expanded_3.csv\",\\\n",
    "         dirpath + \"/cars_expanded_files/cars_expanded_4.csv\",\\\n",
    "         dirpath + \"/cars_expanded_files/cars_expanded_5.csv\",\\\n",
    "         dirpath + \"/cars_expanded_files/cars_expanded_6.csv\",\\\n",
    "         dirpath + \"/cars_expanded_files/cars_expanded_7.csv\"]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"car_1\", StringType(), True), StructField(\"car_2\", StringType(), True),StructField(\"car_3\", StringType(), True),StructField(\"car_4\", StringType(), True),StructField(\"car_5\", StringType(), True),StructField(\"car_6\", StringType(), True),StructField(\"car_7\", StringType(), True),StructField(\"car_8\", StringType(), True),StructField(\"car_9\", StringType(), True),StructField(\"car_10\", StringType(), True)\n",
    "])\n",
    "df_cars_expanded_1 = spark.createDataFrame([], schema)\n",
    "df_cars_expanded_1 = spark.read.option(\"delimiter\", \",\").option(\"header\", \"false\").option(\"quote\",\"\").option(\"inferSchema\", True).csv(paths[0],  schema=schema)\n",
    "\n",
    "df_cars_expanded_2 = spark.createDataFrame([], schema)\n",
    "df_cars_expanded_2 = spark.read.option(\"delimiter\", \",\").option(\"header\", \"false\").option(\"quote\",\"\").option(\"inferSchema\", True).csv(paths[1],  schema=schema)\n",
    "\n",
    "df_cars_expanded_3 = spark.createDataFrame([], schema)\n",
    "df_cars_expanded_3 = spark.read.option(\"delimiter\", \",\").option(\"header\", \"false\").option(\"quote\",\"\").option(\"inferSchema\", True).csv(paths[2],  schema=schema)\n",
    "\n",
    "df_cars_expanded_4 = spark.createDataFrame([], schema)\n",
    "df_cars_expanded_4 = spark.read.option(\"delimiter\", \",\").option(\"header\", \"false\").option(\"quote\",\"\").option(\"inferSchema\", True).csv(paths[3],  schema=schema)\n",
    "\n",
    "df_cars_expanded_5 = spark.createDataFrame([], schema)\n",
    "df_cars_expanded_5 = spark.read.option(\"delimiter\", \",\").option(\"header\", \"false\").option(\"quote\",\"\").option(\"inferSchema\", True).csv(paths[4],  schema=schema)\n",
    "\n",
    "df_cars_expanded_6 = spark.createDataFrame([], schema)\n",
    "df_cars_expanded_6 = spark.read.option(\"delimiter\", \",\").option(\"header\", \"false\").option(\"quote\",\"\").option(\"inferSchema\", True).csv(paths[5],  schema=schema)\n",
    "\n",
    "df_cars_expanded_7 = spark.createDataFrame([], schema)\n",
    "df_cars_expanded_7 = spark.read.option(\"delimiter\", \",\").option(\"header\", \"false\").option(\"quote\",\"\").option(\"inferSchema\", True).csv(paths[6],  schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This extracts the integers from the cell in order to get rid of weird characters\n",
    "for i in range(1,max_size+1): \n",
    "    df_cars_expanded_1 = df_cars_expanded_1.withColumn(\"car_\"+str(i), pyspark.sql.functions.regexp_extract(\"car_\"+str(i), '(\\d+)', 1))\n",
    "    df_cars_expanded_2 = df_cars_expanded_2.withColumn(\"car_\"+str(i), pyspark.sql.functions.regexp_extract(\"car_\"+str(i), '(\\d+)', 1))\n",
    "    df_cars_expanded_3 = df_cars_expanded_3.withColumn(\"car_\"+str(i), pyspark.sql.functions.regexp_extract(\"car_\"+str(i), '(\\d+)', 1))\n",
    "    df_cars_expanded_4 = df_cars_expanded_4.withColumn(\"car_\"+str(i), pyspark.sql.functions.regexp_extract(\"car_\"+str(i), '(\\d+)', 1))\n",
    "    df_cars_expanded_5 = df_cars_expanded_5.withColumn(\"car_\"+str(i), pyspark.sql.functions.regexp_extract(\"car_\"+str(i), '(\\d+)', 1))\n",
    "    df_cars_expanded_6 = df_cars_expanded_6.withColumn(\"car_\"+str(i), pyspark.sql.functions.regexp_extract(\"car_\"+str(i), '(\\d+)', 1))\n",
    "    df_cars_expanded_7 = df_cars_expanded_7.withColumn(\"car_\"+str(i), pyspark.sql.functions.regexp_extract(\"car_\"+str(i), '(\\d+)', 1))\n",
    "    \n",
    "appended = df_cars_expanded_1.union(df_cars_expanded_2) \n",
    "appended = appended.union(df_cars_expanded_3)\n",
    "appended = appended.union(df_cars_expanded_4) \n",
    "appended = appended.union(df_cars_expanded_5) \n",
    "appended = appended.union(df_cars_expanded_6) \n",
    "appended = appended.union(df_cars_expanded_7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This box joins the original dataframe and the recently created one \n",
    "\n",
    "def with_column_index(sdf): \n",
    "    new_schema = StructType(sdf.schema.fields + [StructField(\"ColumnIndex\", LongType(), False),])\n",
    "    return sdf.rdd.zipWithIndex().map(lambda row: row[0] + (row[1],)).toDF(schema=new_schema)\n",
    "\n",
    "df1 = with_column_index(df)\n",
    "df2 = with_column_index(appended)\n",
    "df_expanded = df1.join(df2, df1.ColumnIndex == df2.ColumnIndex, 'inner').drop(\"ColumnIndex\")\n",
    "df_expanded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, all the labels are written in different columns. It is possible to look for one car in particular. \n",
    "# The chosen car is the one with label 182\n",
    "\n",
    "# STUDY OF ONE CAR\n",
    "\n",
    "# This box takes all the timestamps in which a given car is registered\n",
    "# Then, the timestamps are sorted by chronological order \n",
    "df_182 = df_expanded.select(\"timestamp\", \"latitude\", \"longitude\").filter(\"car_1 == '182' OR car_2 == '182'\\\n",
    "                                                                OR car_3 == '182' OR car_4 == '182'\\\n",
    "                                                                OR car_5 == '182' OR car_6 == '182'\\\n",
    "                                                                OR car_7 == '182' OR car_8 == '182'\\\n",
    "                                                                OR car_9 == '182' OR car_10 == '182'\")\n",
    "\n",
    "\n",
    "df_182 = df_182.sort(df_182.timestamp, ascending = True)\n",
    "df_182.show()\n",
    "print(\"The number of timestamps associated with this car is: \"+ str(df_182.count()))\n",
    "\n",
    "# The following plot is a density plot,which gives an idea of the distribution of the locations \n",
    "x = pd.to_numeric(df_182.toPandas()['latitude'], errors='coerce')\n",
    "y = pd.to_numeric(df_182.toPandas()['longitude'], errors='coerce')\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.jointplot(x=x, y=y, kind='kde', color=\"skyblue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, it does not show any temporal distribution. The timestamps, in the way they are registered, are not \n",
    "# suitable for plotting. Therefore, a new column has to be created that register the time as a float number\n",
    "\n",
    "# This piece of code transforms the customized format of the timestamps to a datetime format\n",
    "list_timestamps = []\n",
    "\n",
    "# This piece of code turns the Spark Dataframe into a Pandas Dataframe, which is used for plotting.\n",
    "df_182_pandas = df_182.toPandas() \n",
    "\n",
    "for i in range(len(df_182_pandas['timestamp'])): \n",
    "    new_timestamp = datetime.strptime(df_182_pandas['timestamp'][i], '%Y-%m-%d %H:%M:%S.%f %Z')\n",
    "    list_timestamps.append(new_timestamp)\n",
    "\n",
    "# This piece of transforms the datetime format into floats numbers \n",
    "list_timestamps_float = []\n",
    "for i in range(len(list_timestamps)): \n",
    "    new_timestamp = datetime.timestamp(list_timestamps[i])\n",
    "    list_timestamps_float.append(new_timestamp)\n",
    "\n",
    "# The column of float timestamps is also added   \n",
    "df_182_pandas[\"timestamp_float\"] = list_timestamps_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plot shows the variation of locations along the time\n",
    "\n",
    "data = df_182_pandas\n",
    "fig = px.scatter(data, x= \"latitude\", y=\"longitude\", color= \"timestamp_float\", color_continuous_scale='Magma')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second use case is to analize one particular location and then try to predict the number of cars in it\n",
    "\n",
    "#STUDY OF ONE LOCATION\n",
    "\n",
    "# Registered latitudes \n",
    "df_expanded.groupby('latitude').count().distinct().show()\n",
    "\n",
    "# Registered longitudes \n",
    "df_expanded.groupby('longitude').count().distinct().show()\n",
    "\n",
    "\n",
    "df_location = df_expanded.select(\"timestamp\", \"total_cars\").filter(\"latitude == '32.072323' AND longitude == '34.790555'\")\n",
    "\n",
    "\n",
    "df_location = df_location.sort(df_location.timestamp, ascending = True)\n",
    "df_location.show()\n",
    "print(\"The number of timestamps associated with this location is: \"+ str(df_location.count()))\n",
    "\n",
    "df_location_pandas = df_location.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(y=df_location_pandas.total_cars,\n",
    "                    mode='lines',\n",
    "                    name='Test '))\n",
    "\n",
    "fig.update_layout(title='Temporal evolution of the number of cars in (32.072323, 34.790555)',\n",
    "                   xaxis_title='Timestamp',\n",
    "                   yaxis_title='Number of cars')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save all dataframes for future usage\n",
    "\n",
    "df_182.write.format(\"parquet\").save(\"df_182.parquet\")\n",
    "df_182_pandas.to_pickle(\"df_182_pandas\") \n",
    "df_expanded.write.format(\"parquet\").save(\"df_expanded.parquet\")\n",
    "df_location.write.format(\"parquet\").save(\"df_location.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
