{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook aims to prepare the data for the following model training. Some new feaures, that could be useful \n",
    "# for the training are added. For example, for the dataframe that register the number of cars for a particular \n",
    "# location the time stamps are divided in year, season, month, day, hour and  minutes so the machine learning \n",
    "# algorithm can take advantage of it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "from scipy.stats import kde\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import os \n",
    "dirpath = os.getcwd()\n",
    "\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FEATURE ENGINEERING FOR MACHINE LEARNING ALGORITHM\n",
    "\n",
    "# USE CASE 2: Predict number of cars in a particular location. \n",
    "# The time stamps are divided in year, season, month, day, hour and  minutes so the machine learning \n",
    "# algorithm can take advantage of it\n",
    "\n",
    "df_location = spark.read.load(\"df_location.parquet\")\n",
    "\n",
    "df_location_pandas = df_location.toPandas()\n",
    "\n",
    "list_hours = []\n",
    "list_days = []\n",
    "list_months = []\n",
    "list_seasons = []\n",
    "list_years = []\n",
    "list_minutes = []\n",
    "\n",
    "for i in range(len(df_location_pandas['timestamp'])): \n",
    "    new_timestamp = datetime.strptime(df_location_pandas['timestamp'][i], '%Y-%m-%d %H:%M:%S.%f %Z')\n",
    "    list_minutes.append(new_timestamp.minute)\n",
    "    list_hours.append(new_timestamp.hour)\n",
    "    list_days.append(new_timestamp.day)\n",
    "    list_months.append(new_timestamp.month)\n",
    "    list_years.append(new_timestamp.year)\n",
    "    if new_timestamp.month in range(1,4):\n",
    "        list_seasons.append(int(\"1\"))\n",
    "    elif new_timestamp.month in range(4,7):\n",
    "        list_seasons.append(int(\"2\"))\n",
    "    elif new_timestamp.month in range(7,10):\n",
    "        list_seasons.append(int(\"3\"))\n",
    "    else:\n",
    "        list_seasons.append(int(\"4\"))\n",
    "\n",
    "df_location_pandas[\"minute\"] = list_minutes\n",
    "df_location_pandas[\"hour\"] = list_hours\n",
    "df_location_pandas[\"day\"] = list_days\n",
    "df_location_pandas[\"month\"] = list_months\n",
    "df_location_pandas[\"season\"] = list_seasons\n",
    "df_location_pandas[\"year\"] = list_years\n",
    "\n",
    "df_location_pandas[\"total_cars_int\"] = df_location_pandas.total_cars.astype(int)\n",
    "\n",
    "df_location_expanded = spark.createDataFrame(df_location_pandas)\n",
    "df_location_expanded.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statisctics, just for have some idea about possible correlations\n",
    "\n",
    "df_location_pandas.corr(method ='pearson') \n",
    "\n",
    "# Preparation of the dataframe for the training. The features column is added\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=['minute','hour','day','month','season','year'],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "df_location_ml = vectorAssembler.transform(df_location_expanded)\n",
    "\n",
    "df_location_ml.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframes for a later usage\n",
    "vectorAssembler.save(dirpath + \"/vector_assembler\")\n",
    "\n",
    "df_location_expanded.write.format(\"parquet\").save(\"df_location_expanded.parquet\")\n",
    "df_location_ml.write.format(\"parquet\").save(\"df_location_ml.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING FOR DEEP LEARNING ALGORITHM \n",
    "\n",
    "df_location_expanded = spark.read.load(\"df_location_expanded.parquet\")\n",
    "\n",
    "df_pandas = df_location_expanded.toPandas()\n",
    "df_pandas = df_pandas.sort_values(by='timestamp',ascending=True)\n",
    "df_pandas = df_pandas.reset_index(drop=True)\n",
    "df = df_pandas.drop(['timestamp','total_cars'], axis = 1)\n",
    "\n",
    "X = df.iloc[:, 0:6].values\n",
    "y = df.iloc[:, 6].values\n",
    "\n",
    "# Dummy variable for season (Because fall is not more than Spring)\n",
    "onehotencoder = OneHotEncoder(categorical_features = [4]) #4 is the number of the column \n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)\n",
    "\n",
    "# Saving X_train, X_test, y_train, y_test, X_train_sc and X_test_sc to csv file\n",
    "savetxt('X_train.csv', X_train, delimiter=',')\n",
    "savetxt('X_test.csv', X_test, delimiter=',')\n",
    "savetxt('y_train.csv', y_train, delimiter=',')\n",
    "savetxt('y_test.csv', y_test, delimiter=',')\n",
    "savetxt('X_train_sc.csv', X_train_sc, delimiter=',')\n",
    "savetxt('X_test_sc.csv', X_test_sc, delimiter=',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
