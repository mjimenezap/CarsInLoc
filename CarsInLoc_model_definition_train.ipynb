{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "\n",
    "import os \n",
    "dirpath = os.getcwd()\n",
    "\n",
    "from numpy import loadtxt\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MACHINE LEARNING MODEL\n",
    "\n",
    "# Load the model\n",
    "df_location_ml = spark.read.load(\"df_location_ml.parquet\")\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler.load(dirpath + \"/vector_assembler\")\n",
    " \n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "classifier_ml =  DecisionTreeRegressor(labelCol='total_cars_int', featuresCol='features')\n",
    "\n",
    "df_location_ml_red = df_location_ml.select(['total_cars_int','minute' ,'hour','day','month','season','year'])\n",
    "\n",
    "splits = df_location_ml_red.randomSplit([0.7, 0.3])\n",
    "ml_train_df = splits[0]\n",
    "ml_test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vectorAssembler, classifier_ml])\n",
    "\n",
    "model_ml = pipeline.fit(ml_train_df)\n",
    "\n",
    "model_ml.save(dirpath + \"/ML_model\")\n",
    "\n",
    "ml_train_df.write.format(\"parquet\").save(\"ml_train_df.parquet\")\n",
    "ml_test_df.write.format(\"parquet\").save(\"ml_test_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the importance of each feature in the model\n",
    "\n",
    "# Code from https://www.timlrx.com/2018/06/19/feature-selection-using-feature-importance-score-creating-a-pyspark-estimator/\n",
    "\n",
    "treeModel = model_ml.stages[-1]\n",
    "print(treeModel.featureImportances)\n",
    "\n",
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "\n",
    "ExtractFeatureImp(treeModel.featureImportances, df_location_ml, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEP LEARNING MODEL\n",
    "\n",
    "# Load numpy arrays from csv files\n",
    "X_train_sc = loadtxt('X_train_sc.csv', delimiter=',')\n",
    "y_train = loadtxt('y_train.csv', delimiter=',')\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Neural Network\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer. Input_dim is 6 because I have 7 columns\n",
    "classifier.add(Dense(output_dim = 24, init = 'he_uniform', activation = 'relu', input_dim = 7))\n",
    "\n",
    "classifier.add(Dense(output_dim = 12, init = 'he_uniform', activation = 'relu'))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'he_uniform', activation = 'tanh'))\n",
    "#classifier.add(Dense(output_dim = 4, init = 'he_uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'he_uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling Neural Network\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting our model \n",
    "classifier.fit(X_train_sc, y_train, batch_size = 10, nb_epoch = 10)\n",
    "\n",
    "DL_model_json = classifier.to_json()\n",
    "with open(\"DL_model.json\", \"w\") as json_file:\n",
    "    json_file.write(DL_model_json)\n",
    "# serialize weights to HDF5\n",
    "classifier.save_weights(\"DL_model_weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
